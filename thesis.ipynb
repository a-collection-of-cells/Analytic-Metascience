{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df94ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 34 citing papers to: C:\\Users\\Karthik\\Desktop\\MSc Project\\data\\citing_papers_20250913_140840.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>openalex_id</th>\n",
       "      <th>is_oa</th>\n",
       "      <th>oa_status</th>\n",
       "      <th>oa_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analogy Powered by Prediction and Structural I...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://doi.org/10.1021/jacs.2c02653</td>\n",
       "      <td>https://openalex.org/W4281888261</td>\n",
       "      <td>True</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>https://doi.org/10.1021/jacs.2c02653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average minimum distances of periodic point se...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://doi.org/10.46793/match.87-3.529w</td>\n",
       "      <td>https://openalex.org/W3214916886</td>\n",
       "      <td>True</td>\n",
       "      <td>bronze</td>\n",
       "      <td>https://match.pmf.kg.ac.rs/electronic_versions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recognizing Rigid Patterns of Unlabeled Point ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://doi.org/10.1109/cvpr52729.2023.00129</td>\n",
       "      <td>https://openalex.org/W4386076138</td>\n",
       "      <td>True</td>\n",
       "      <td>green</td>\n",
       "      <td>https://arxiv.org/pdf/2303.15385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Material Property Prediction Using Graphs Base...</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://doi.org/10.1007/s40192-024-00351-9</td>\n",
       "      <td>https://openalex.org/W4394843284</td>\n",
       "      <td>True</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>https://link.springer.com/content/pdf/10.1007/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generic families of finite metric spaces with ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://doi.org/10.1007/s41468-024-00177-6</td>\n",
       "      <td>https://openalex.org/W4396719438</td>\n",
       "      <td>True</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>https://link.springer.com/content/pdf/10.1007/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year  \\\n",
       "0  Analogy Powered by Prediction and Structural I...  2022   \n",
       "1  Average minimum distances of periodic point se...  2021   \n",
       "2  Recognizing Rigid Patterns of Unlabeled Point ...  2023   \n",
       "3  Material Property Prediction Using Graphs Base...  2024   \n",
       "4  Generic families of finite metric spaces with ...  2024   \n",
       "\n",
       "                                            doi  \\\n",
       "0          https://doi.org/10.1021/jacs.2c02653   \n",
       "1      https://doi.org/10.46793/match.87-3.529w   \n",
       "2  https://doi.org/10.1109/cvpr52729.2023.00129   \n",
       "3    https://doi.org/10.1007/s40192-024-00351-9   \n",
       "4    https://doi.org/10.1007/s41468-024-00177-6   \n",
       "\n",
       "                        openalex_id  is_oa oa_status  \\\n",
       "0  https://openalex.org/W4281888261   True    hybrid   \n",
       "1  https://openalex.org/W3214916886   True    bronze   \n",
       "2  https://openalex.org/W4386076138   True     green   \n",
       "3  https://openalex.org/W4394843284   True    hybrid   \n",
       "4  https://openalex.org/W4396719438   True    hybrid   \n",
       "\n",
       "                                              oa_url  \n",
       "0               https://doi.org/10.1021/jacs.2c02653  \n",
       "1  https://match.pmf.kg.ac.rs/electronic_versions...  \n",
       "2                   https://arxiv.org/pdf/2303.15385  \n",
       "3  https://link.springer.com/content/pdf/10.1007/...  \n",
       "4  https://link.springer.com/content/pdf/10.1007/...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET SAVED] {'openalex_id': 'W3162861102', 'doi': '10.1007/978-3-030-76657-3_16', 'title': 'An Isometry Classification of Periodic Point Sets', 'year': 2021, 'first_author': 'Olga Anosova'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Retrieve All Citing Papers Using OpenAlex API (Paginated Version)\n",
    "Description: Input by DOI or Title, retrieves citing papers with open access metadata,\n",
    "saves results to a timestamped CSV, and stores pointer files for reuse in downstream steps.\n",
    "Author: Karthik Anand Balasubramanian\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os, time, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display  # for display(df.head())\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "EMAIL = \"karthik.nb@yahoo.com\"\n",
    "BASE_URL = \"https://api.openalex.org\"\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\Karthik\\Desktop\\MSc Project\\data\"  # CSVs & pointers live here\n",
    "DATA_DIR = Path(OUTPUT_FOLDER)\n",
    "\n",
    "def get_openalex_id_from_title(title: str):\n",
    "    \"\"\"\n",
    "    Search for a paper on OpenAlex using its title.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the paper to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - str: OpenAlex Work ID (without URL prefix) if found, else None.\n",
    "            - dict: Full OpenAlex JSON metadata for the best-matching paper, else None.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/works\"\n",
    "    params = {\"search\": title, \"per_page\": 1, \"mailto\": EMAIL}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    results = r.json().get(\"results\", [])\n",
    "    if not results:\n",
    "        return None, None\n",
    "    rec = results[0]\n",
    "    return rec[\"id\"].split(\"/\")[-1], rec\n",
    "\n",
    "def fetch_work_by_openalex_id(openalex_id: str):\n",
    "    \"\"\"\n",
    "    Fetch the full OpenAlex metadata for a paper using its Work ID.\n",
    "\n",
    "    Args:\n",
    "        openalex_id (str): OpenAlex Work ID (e.g., 'W123456789').\n",
    "\n",
    "    Returns:\n",
    "        dict: Full metadata JSON from OpenAlex.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/works/{openalex_id}\"\n",
    "    r = requests.get(url, params={\"mailto\": EMAIL}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_all_citing_papers(openalex_id: str):\n",
    "    \"\"\"\n",
    "    Retrieve all papers that cite a given OpenAlex Work ID, using cursor-based pagination.\n",
    "\n",
    "    Args:\n",
    "        openalex_id (str): OpenAlex Work ID of the target paper.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of citing paper metadata records from OpenAlex.\n",
    "    \"\"\"\n",
    "    all_results, cursor, per_page = [], \"*\", 25\n",
    "    while True:\n",
    "        url = f\"{BASE_URL}/works\"\n",
    "        params = {\"filter\": f\"cites:{openalex_id}\", \"per_page\": per_page, \"cursor\": cursor, \"mailto\": EMAIL}\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            print(\"Error fetching data:\", r.status_code)\n",
    "            break\n",
    "        data = r.json()\n",
    "        all_results.extend(data.get(\"results\", []))\n",
    "        cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "        time.sleep(1)  # polite delay\n",
    "    return all_results\n",
    "\n",
    "def save_to_csv(papers, filename_prefix=\"citing_papers\", output_folder=OUTPUT_FOLDER):\n",
    "    \"\"\"\n",
    "    Save a list of citing paper metadata to a timestamped CSV file.\n",
    "\n",
    "    Args:\n",
    "        papers (list[dict]): List of citing papers' metadata dictionaries.\n",
    "        filename_prefix (str): Prefix for the generated CSV filename.\n",
    "        output_folder (str): Directory to save the CSV.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pandas.DataFrame: The saved dataframe.\n",
    "            - str: Full path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "    full_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    records = []\n",
    "    for p in papers:\n",
    "        records.append({\n",
    "            \"title\": p.get(\"title\"),\n",
    "            \"year\": p.get(\"publication_year\"),\n",
    "            \"doi\": p.get(\"doi\"),\n",
    "            \"openalex_id\": p.get(\"id\"),\n",
    "            \"is_oa\": (p.get(\"open_access\") or {}).get(\"is_oa\"),\n",
    "            \"oa_status\": (p.get(\"open_access\") or {}).get(\"oa_status\"),\n",
    "            \"oa_url\": (p.get(\"open_access\") or {}).get(\"oa_url\"),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"\\nSaved {len(df)} citing papers to: {full_path}\")\n",
    "    return df, full_path\n",
    "\n",
    "def _get_first_author(paper_json):\n",
    "    \"\"\"\n",
    "    Extract the first author's name from OpenAlex metadata.\n",
    "\n",
    "    Args:\n",
    "        paper_json (dict): Full OpenAlex JSON record for a paper.\n",
    "\n",
    "    Returns:\n",
    "        str: First author display name, or empty string if unavailable.\n",
    "    \"\"\"\n",
    "    auths = paper_json.get(\"authorships\") or []\n",
    "    if not auths:\n",
    "        return \"\"\n",
    "    return (auths[0].get(\"author\") or {}).get(\"display_name\", \"\") or \"\"\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "choice = input(\"Search using (1) DOI or (2) Title? Enter 1 or 2: \").strip()\n",
    "\n",
    "selected_paper = None\n",
    "openalex_id = None\n",
    "\n",
    "if choice == \"1\":\n",
    "    doi = input(\"Enter DOI of the target paper: \").strip()\n",
    "    # Works-by-DOI returns the full record directly\n",
    "    url = f\"{BASE_URL}/works/https://doi.org/{doi}\"\n",
    "    r = requests.get(url, params={\"mailto\": EMAIL}, timeout=30)\n",
    "    if r.status_code == 200:\n",
    "        selected_paper = r.json()\n",
    "        openalex_id = selected_paper[\"id\"].split(\"/\")[-1]\n",
    "    else:\n",
    "        print(\"DOI lookup failed:\", r.status_code)\n",
    "\n",
    "elif choice == \"2\":\n",
    "    title = input(\"Enter the title of the target paper: \").strip()\n",
    "    openalex_id, selected_paper = get_openalex_id_from_title(title)\n",
    "    if openalex_id and not selected_paper:\n",
    "        # Fallback: fetch full record if search helper didn't return it\n",
    "        selected_paper = fetch_work_by_openalex_id(openalex_id)\n",
    "else:\n",
    "    print(\"Invalid input. Exiting.\")\n",
    "\n",
    "if openalex_id:\n",
    "    citing_papers = get_all_citing_papers(openalex_id)\n",
    "    df, output_csv_path = save_to_csv(citing_papers, output_folder=OUTPUT_FOLDER)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Could not retrieve citing papers due to missing or invalid ID.\")\n",
    "\n",
    "# --- Save a pointer to the latest CSV and the selected target ---\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if openalex_id and selected_paper:\n",
    "    # 1) Save path to latest CSV file\n",
    "    (DATA_DIR / \"latest_citing_papers_path.txt\").write_text(str(output_csv_path), encoding=\"utf-8\")\n",
    "\n",
    "    # 2) Save metadata for the target paper for later pipeline steps\n",
    "    target_meta = {\n",
    "        \"openalex_id\": (selected_paper.get(\"id\") or \"\").split(\"/\")[-1],\n",
    "        \"doi\": (selected_paper.get(\"doi\") or \"\").replace(\"https://doi.org/\", \"\") or None,\n",
    "        \"title\": selected_paper.get(\"title\"),\n",
    "        \"year\": selected_paper.get(\"publication_year\"),\n",
    "        \"first_author\": _get_first_author(selected_paper),\n",
    "    }\n",
    "    (DATA_DIR / \"target_work.json\").write_text(json.dumps(target_meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(\"[TARGET SAVED]\", target_meta)\n",
    "else:\n",
    "    print(\"[WARN] target not saved (missing openalex_id or selected_paper).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8941d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer file exists: True\n",
      "CSV path from pointer: C:\\Users\\Karthik\\Desktop\\MSc Project\\data\\citing_papers_20250913_140840.csv\n",
      "CSV file exists: True\n",
      "Columns in CSV: ['title', 'year', 'doi', 'openalex_id', 'is_oa', 'oa_status', 'oa_url']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "pointer_path = Path(r\"C:\\Users\\Karthik\\Desktop\\MSc Project\\data\\latest_citing_papers_path.txt\")\n",
    "print(\"Pointer file exists:\", pointer_path.exists())\n",
    "\n",
    "if pointer_path.exists():\n",
    "    csv_path = Path(pointer_path.read_text().strip())\n",
    "    print(\"CSV path from pointer:\", csv_path)\n",
    "    print(\"CSV file exists:\", csv_path.exists())\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path, nrows=1)\n",
    "        print(\"Columns in CSV:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d104fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] Using latest citing CSV: C:\\Users\\Karthik\\Desktop\\MSc Project\\data\\citing_papers_20250913_140840.csv\n",
      "Rows in CSV: 34 | candidates: 32  (OA+URL=29, DOI-only=2)\n",
      "[PDF SAVED via DOI landing scrape] downloaded_files_2025-09-13_14-09-30\\Analogy Powered by Prediction and Structural Invariants_ Computationally Led Discovery of a Mesoporous Hydrogen-Bonded O.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Average minimum distances of periodic point sets – foundational invariants for mapping periodic crystals.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negati.pdf\n",
      "[UNPAYWALL LOCATION SAVED] downloaded_files_2025-09-13_14-09-30\\Material Property Prediction Using Graphs Based on Generically Complete Isometry Invariants.pdf\n",
      "[UNPAYWALL LANDING → PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Generic families of finite metric spaces with identical or trivial 1-dimensional persistence.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Fast Predictions of Lattice Energies by Continuous Isometry Invariants of Crystal Structures.pdf\n",
      "[UNPAYWALL LOCATION SAVED] downloaded_files_2025-09-13_14-09-30\\Mathematics of 2-Dimensional Lattices.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\The importance of definitions in crystallography.pdf\n",
      "[UNPAYWALL LOCATION SAVED] downloaded_files_2025-09-13_14-09-30\\Density Functions of Periodic Sequences of Continuous Events.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Polynomial-Time Algorithms for Continuous Metrics on Atomic Clouds of Unordered Points.pdf\n",
      "[PDF SAVED via DOI landing scrape] downloaded_files_2025-09-13_14-09-30\\Geographic-style maps with a local novelty distance help navigate in the materials space.pdf\n",
      "[PDF FOUND IN HTML & SAVED] downloaded_files_2025-09-13_14-09-30\\Continuous Invariant-Based Maps of the Cambridge Structural Database.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Geographic style maps for two-dimensional lattices.pdf\n",
      "[GAVE UP] Couldn’t fetch PDF via DOI workflow for 10.1007/978-3-031-20713-6_29\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006.pdf\n",
      "[UNPAYWALL LANDING → PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Bounds for the Regularity Radius of Delone Sets.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Continuous chiral distances for two‐dimensional lattices.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Entropic Trust Region for Densest Crystallographic Symmetry Group Packings.pdf\n",
      "[GAVE UP] Couldn’t fetch PDF via DOI workflow for 10.3390/math9172121\n",
      "[PDF FOUND IN HTML & SAVED] downloaded_files_2025-09-13_14-09-30\\Families of point sets with identical 1D persistence.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Recognition of Near-Duplicate Periodic Patterns by Continuous Metrics with Approximation Guarantees.pdf\n",
      "[GAVE UP] Couldn’t fetch PDF via DOI workflow for 10.48550/arxiv.2212.11246\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Introduction to invariant-based machine learning for periodic crystals.pdf\n",
      "[GAVE UP] Couldn’t fetch PDF via DOI workflow for 10.1016/j.cpc.2023.108889\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Potential materials genome for mapping the continuous space of all periodic crystals.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\A continuous map of 2.6+ million 2D lattices from the Cambridge Structural Database.pdf\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\Continuous maps of molecules and atomic clouds in large databases.pdf\n",
      "[GAVE UP] Couldn’t fetch PDF via DOI workflow for 10.1134/s0965542522080024\n",
      "[PDF SAVED] downloaded_files_2025-09-13_14-09-30\\A unique and continuous code of all periodic crystals.pdf\n",
      "\n",
      " Done downloading files to: downloaded_files_2025-09-13_14-09-30\n",
      "Summary: PDFs saved = 25, manual downloads logged = 5\n",
      "\n",
      "=== Processing manual uploads ===\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2 (FINAL, patched): Download OA PDFs for citing papers\n",
    "- Robust against 403s (MDPI/Wiley fixes), uses Unpaywall early for strict publishers,\n",
    "  retries with a single Session, validates saved PDFs, and logs manual fetches as JSONL.\n",
    "\n",
    "Inputs (from Step 1 CSV):\n",
    "    title, year, doi, openalex_id, is_oa, oa_status, oa_url\n",
    "\n",
    "Outputs:\n",
    "    downloaded_files_<timestamp>/...  (PDFs)\n",
    "    manual_download_required.txt      (JSONL: items to fetch manually)\n",
    "    latest_download_dir.txt           (pointer for Step 3 auto-detect)\n",
    "\"\"\"\n",
    "\n",
    "# ======= IMPORTS =======\n",
    "import os, re, time, random, shutil, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, quote, urlparse\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ======= CONFIGURATION =======\n",
    "DATA_DIR = Path(r\"C:\\Users\\Karthik\\Desktop\\MSc Project\\data\")\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path.cwd() / \"data\"\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "POINTER = DATA_DIR / \"latest_citing_papers_path.txt\"  # written by Step 1\n",
    "REQUIRED_COLS = {\"title\",\"year\",\"doi\",\"openalex_id\",\"is_oa\",\"oa_status\",\"oa_url\"}\n",
    "\n",
    "YOUR_EMAIL = \"karthik.nb@yahoo.com\"  # REQUIRED for Unpaywall/OpenAlex etiquette\n",
    "if \"@\" not in YOUR_EMAIL:\n",
    "    print(\"[WARN] Set YOUR_EMAIL to a real address for Unpaywall/OpenAlex etiquette.\")\n",
    "\n",
    "BROWSER_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Strict pacing/strategies per host\n",
    "STRICT_DOMAINS = {\"pubs.acs.org\", \"onlinelibrary.wiley.com\", \"tandfonline.com\", \"mdpi.com\"}\n",
    "STRICT_PUBS     = (\"mdpi.com\",\"onlinelibrary.wiley.com\",\"sciencedirect.com\",\"elsevier.com\",\"springer.com\")\n",
    "\n",
    "DEMO = False    # set True during recording to cap runtime\n",
    "DEMO_MAX = 12   # max rows to attempt when DEMO=True\n",
    "\n",
    "# ======= SESSION (reused) with retries =======\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.8,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\",\"GET\",\"OPTIONS\"]\n",
    "    )\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(BROWSER_HEADERS.copy())\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# ======= HELPERS =======\n",
    "def safe_str(x):\n",
    "    s = str(x or \"\").strip()\n",
    "    return None if s.lower() in {\"\", \"nan\", \"none\", \"null\"} else s\n",
    "\n",
    "def _find_latest_citing_csv(data_dir: Path, pointer: Path) -> Path:\n",
    "    # 1) Prefer pointed file if present and valid\n",
    "    if pointer.exists():\n",
    "        p = Path(pointer.read_text(encoding=\"utf-8\").strip())\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # 2) Else pick newest matching file in data_dir (then CWD)\n",
    "    patterns = [\"citing_papers_*.csv\", \"*citing*papers*.csv\"]\n",
    "    candidates = []\n",
    "    for pat in patterns:\n",
    "        candidates.extend(data_dir.glob(pat))\n",
    "    if not candidates:\n",
    "        candidates = list(Path(\".\").glob(\"citing_papers_*.csv\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No citing_papers CSV found in {data_dir} or current directory.\")\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    # 3) Choose newest with expected columns\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            cols = {c.strip().lower() for c in pd.read_csv(p, nrows=0).columns}\n",
    "            if REQUIRED_COLS.issubset(cols):\n",
    "                return p\n",
    "        except Exception:\n",
    "            continue\n",
    "    return candidates[0]\n",
    "\n",
    "def sanitize_filename(title: str) -> str:\n",
    "    t = (title or \"\").strip()\n",
    "    t = re.sub(r\"[\\\\/:*?\\\"<>|\\r\\n]+\", \"_\", t)\n",
    "    t = re.sub(r\"_+\", \"_\", t).strip(\"_\")\n",
    "    return (t[:120] or \"paper\")\n",
    "\n",
    "def normalize_doi(doi):\n",
    "    #remove redundant part of url for doi\n",
    "    if not doi:\n",
    "        return None\n",
    "    d = doi.strip()\n",
    "    if d.lower().startswith(\"http\"):\n",
    "        d = d.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\").replace(\"doi.org/\", \"\")\n",
    "    return d.strip() or None\n",
    "\n",
    "def _resolve_pdf_links_from_html(html: str, base_url: str) -> list:\n",
    "    \n",
    "    links = re.findall(r'href=[\"\\']([^\"\\']+\\.pdf[^\"\\']*)[\"\\']', html, flags=re.I)\n",
    "    links += re.findall(r'href=[\"\\']([^\"\\']+(?:/pdf|format=pdf|type=pdf)[^\"\\']*)[\"\\']', html, flags=re.I)\n",
    "    out = []\n",
    "    for href in links:\n",
    "        full = urljoin(base_url, href)\n",
    "        if full not in out:\n",
    "            out.append(full)\n",
    "    return out\n",
    "\n",
    "def domain_sleep(url: str):\n",
    "    try:\n",
    "        host = urlparse(url).hostname or \"\"\n",
    "    except Exception:\n",
    "        host = \"\"\n",
    "    if any(h in host for h in STRICT_DOMAINS):\n",
    "        time.sleep(3 + random.random()*2)  # 3–5s\n",
    "    else:\n",
    "        time.sleep(0.5 + random.random()*0.5)\n",
    "\n",
    "MIN_BYTES = 30_000\n",
    "def is_valid_pdf(path: str) -> bool:\n",
    "    #Checks the file exists, is at least 30KB (filters junk), and starts with %PDF- (magic header). Prevents saving HTML as .pdf.\n",
    "    try:\n",
    "        if not os.path.exists(path): return False\n",
    "        if os.path.getsize(path) < MIN_BYTES: return False\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(5)\n",
    "        return head == b\"%PDF-\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _stream_save_resp_to_pdf(resp: requests.Response, dest_path: str) -> bool:\n",
    "    #Verifies the server is actually sending a PDF (or an octet-stream + .pdf URL). Returns True if it looked like a PDF based on headers/URL.\n",
    "    \n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    # Accept octet-stream if URL ends with .pdf\n",
    "    if (\"pdf\" not in ctype) and (\"octet-stream\" not in ctype) and (not resp.url.lower().endswith(\".pdf\")):\n",
    "        return False\n",
    "    with open(dest_path, \"wb\") as f:\n",
    "        for chunk in resp.iter_content(1 << 14):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return True\n",
    "\n",
    "def save_and_validate(resp: requests.Response, dest_path: str) -> bool:\n",
    "    \n",
    "    \"\"\"Writes the response with _stream_save_resp_to_pdf, then validates with is_valid_pdf. If invalid, it removes the file. \n",
    "    Returns True only if all checks pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not _stream_save_resp_to_pdf(resp, dest_path):\n",
    "        return False\n",
    "    if not is_valid_pdf(dest_path):\n",
    "        try: os.remove(dest_path)\n",
    "        except Exception: pass\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _download_pdf_with_headers(session: requests.Session, pdf_url: str, dest_path: str, referer: str | None) -> bool:\n",
    "    \"\"\"Requests the pdf_url with Accept: application/pdf. If referer is provided, it adds it (some sites require a valid referrer). \n",
    "       Then calls save_and_validate.\n",
    "    \"\"\"\n",
    "    headers = {\"Accept\": \"application/pdf\"}\n",
    "    if referer: headers[\"Referer\"] = referer\n",
    "    r = session.get(pdf_url, headers=headers, timeout=30, stream=True, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    return save_and_validate(r, dest_path)\n",
    "\n",
    "def mdpi_fix(u: str) -> str:\n",
    "    #MDPI often requires download=1 in the query string for direct PDF. This function appends it when appropriate.\n",
    "    try:\n",
    "        host = urlparse(u).hostname or \"\"\n",
    "    except Exception:\n",
    "        host = \"\"\n",
    "    if \"mdpi.com\" in host and \"pdf\" in u and \"download=\" not in u:\n",
    "        sep = \"&\" if \"?\" in u else \"?\"\n",
    "        return f\"{u}{sep}download=1\"\n",
    "    return u\n",
    "\n",
    "def try_wiley_epdf(session: requests.Session, doi: str, dest_path: str) -> bool:\n",
    "    \"\"\"Wiley sometimes serves PDFs via an ePDF viewer page. This function loads the ePDF page, scrapes it for actual PDF links, \n",
    "    tries to download those PDFs with the correct Referer. Returns True if it manages to save a valid PDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not doi: return False\n",
    "    epdf = f\"https://onlinelibrary.wiley.com/doi/epdf/{doi}\"\n",
    "    try:\n",
    "        r = session.get(epdf, headers={\"Referer\":\"https://onlinelibrary.wiley.com/\"}, timeout=25, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return False\n",
    "        links = _resolve_pdf_links_from_html(r.text, r.url)\n",
    "        for l in links:\n",
    "            try:\n",
    "                if _download_pdf_with_headers(session, l, dest_path, referer=r.url):\n",
    "                    print(f\"[WILEY EPDF → PDF SAVED] {dest_path}\")\n",
    "                    return True\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def prefer_unpaywall_first(url: str | None) -> bool:\n",
    "    #Returns True if the OA URL looks like it belongs to a strict publisher. \n",
    "    #If so, the code will try Unpaywall first (repositories are often easier/freer to fetch).\n",
    "\n",
    "    if not url: return False\n",
    "    try:\n",
    "        host = urlparse(url).hostname or \"\"\n",
    "    except Exception:\n",
    "        return False\n",
    "    return any(h in host for h in STRICT_PUBS)\n",
    "\n",
    "def get_unpaywall_candidate_pdfs(session: requests.Session, doi: str | None, email=YOUR_EMAIL) -> list:\n",
    "    #Calls Unpaywall API for the DOI.\n",
    "    # Collects repository url_for_pdf first (best), then repository url, then non-repo url_for_pdf, then non-repo url.\n",
    "    # Deduplicates and returns a list (repository first). (Works without paywalls.)\n",
    "    \n",
    "    if not doi:\n",
    "        return []\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=20)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        j = r.json()\n",
    "    except Exception:\n",
    "        return []\n",
    "    locs = j.get(\"oa_locations\") or []\n",
    "    def is_repo(loc): return (loc or {}).get(\"host_type\") == \"repository\"\n",
    "    def get_pdf(loc): return (loc or {}).get(\"url_for_pdf\")\n",
    "    def get_url(loc): return (loc or {}).get(\"url\")\n",
    "\n",
    "    cands = []\n",
    "    for loc in locs:\n",
    "        if is_repo(loc) and get_pdf(loc): cands.append(get_pdf(loc))\n",
    "    for loc in locs:\n",
    "        if is_repo(loc) and get_url(loc): cands.append(get_url(loc))\n",
    "    for loc in locs:\n",
    "        if (not is_repo(loc)) and get_pdf(loc): cands.append(get_pdf(loc))\n",
    "    for loc in locs:\n",
    "        if (not is_repo(loc)) and get_url(loc): cands.append(get_url(loc))\n",
    "\n",
    "    out, seen = [], set()\n",
    "    for u in cands:\n",
    "        if u and (u not in seen):\n",
    "            seen.add(u); out.append(u)\n",
    "    return out\n",
    "\n",
    "def get_openalex_repository_pdfs(session: requests.Session, openalex_id: str | None, email=YOUR_EMAIL) -> list:\n",
    "    #Calls OpenAlex for the work, looks at primary_location + locations, and collects pdf_url only where host_type == \"repository\". Returns a deduped list.\n",
    "    \n",
    "    if not openalex_id:\n",
    "        return []\n",
    "    wid = openalex_id.split(\"/\")[-1] if \"/\" in openalex_id else openalex_id\n",
    "    try:\n",
    "        j = session.get(\n",
    "            f\"https://api.openalex.org/works/{wid}\",\n",
    "            params={\"mailto\": email}, timeout=20\n",
    "        ).json()\n",
    "    except Exception:\n",
    "        return []\n",
    "    locs = []\n",
    "    if isinstance(j.get(\"primary_location\"), dict): locs.append(j[\"primary_location\"])\n",
    "    locs.extend(j.get(\"locations\") or [])\n",
    "    out, seen = [], set()\n",
    "    for loc in locs:\n",
    "        if isinstance(loc, dict) and loc.get(\"host_type\") == \"repository\":\n",
    "            u = loc.get(\"pdf_url\") or \"\"\n",
    "            if u and u not in seen:\n",
    "                seen.add(u); out.append(u)\n",
    "    return out\n",
    "\n",
    "def download_pdf_from_doi(session: requests.Session, doi_or_url: str, dest_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Robust DOI→PDF:\n",
    "      1) DOI content-negotiation (Accept: application/pdf)\n",
    "      2) Resolve landing and scrape PDF links\n",
    "      3) Try publisher direct PDF endpoints (ACS/Springer/Wiley/T&F)\n",
    "      4) Landing again with Accept: application/pdf\n",
    "    \"\"\"\n",
    "    plain_doi = normalize_doi(doi_or_url)\n",
    "    if not plain_doi:\n",
    "        return False\n",
    "    doi_url = f\"https://doi.org/{plain_doi}\"\n",
    "\n",
    "    # 1) Content negotiation (direct pdf download if possible)\n",
    "    try:\n",
    "        r = session.get(doi_url, headers={\"Accept\":\"application/pdf\"}, timeout=25, allow_redirects=True, stream=True)\n",
    "        if r.status_code == 200 and save_and_validate(r, dest_path):\n",
    "            print(f\"[PDF SAVED via DOI content-negotiation] {dest_path}\")\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) If HTML, scrape for PDF links\n",
    "    landing_url, html = None, None\n",
    "    try:\n",
    "        r = session.get(doi_url, timeout=25, allow_redirects=True)\n",
    "        r.raise_for_status()\n",
    "        landing_url, html = r.url, r.text\n",
    "    except Exception:\n",
    "        landing_url, html = None, None\n",
    "\n",
    "    if html and landing_url:\n",
    "        for pdf_url in _resolve_pdf_links_from_html(html, landing_url):\n",
    "            try:\n",
    "                if _download_pdf_with_headers(session, pdf_url, dest_path, referer=landing_url):\n",
    "                    print(f\"[PDF SAVED via DOI landing scrape] {dest_path}\")\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 3) try known direct PDF URL shapes for ACS, Springer, Wiley, Taylor & Francis.\n",
    "    candidates = []\n",
    "    # ACS\n",
    "    candidates += [f\"https://pubs.acs.org/doi/pdf/{plain_doi}\",\n",
    "                   f\"https://pubs.acs.org/doi/pdfdirect/{plain_doi}\"]\n",
    "    # Springer\n",
    "    springer_doi = quote(plain_doi, safe=\"\")\n",
    "    candidates += [f\"https://link.springer.com/content/pdf/{plain_doi}.pdf\",\n",
    "                   f\"https://link.springer.com/content/pdf/{springer_doi}.pdf\"]\n",
    "    # Wiley\n",
    "    candidates += [f\"https://onlinelibrary.wiley.com/doi/pdfdirect/{plain_doi}?download=true\",\n",
    "                   f\"https://onlinelibrary.wiley.com/doi/pdf/{plain_doi}\"]\n",
    "    # Taylor & Francis\n",
    "    candidates += [f\"https://www.tandfonline.com/doi/pdf/{plain_doi}?needAccess=true\"]\n",
    "\n",
    "    for cand in candidates:\n",
    "        try:\n",
    "            resp = session.get(cand, headers={\"Accept\":\"application/pdf\"}, timeout=25, allow_redirects=True, stream=True)\n",
    "            if resp.status_code == 200 and save_and_validate(resp, dest_path):\n",
    "                print(f\"[PDF SAVED via publisher pattern] {cand} -> {dest_path}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Try landing again with Accept: application/pdf (sometimes serves PDF on second try with the right header).\n",
    "    if landing_url:\n",
    "        try:\n",
    "            r2 = session.get(landing_url, headers={\"Accept\":\"application/pdf\"}, timeout=25, allow_redirects=True, stream=True)\n",
    "            if r2.status_code == 200 and save_and_validate(r2, dest_path):\n",
    "                print(f\"[PDF SAVED via landing accept] {dest_path}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"[GAVE UP] Couldn’t fetch PDF via DOI workflow for {plain_doi}\")\n",
    "    return False\n",
    "\n",
    "def log_manual_download(row: pd.Series, output_dir: str):\n",
    "    #to-do list for manual fetching\n",
    "    \n",
    "    log_path = os.path.join(output_dir, \"manual_download_required.txt\")\n",
    "    fields = {\n",
    "        \"title\": row.get(\"title\",\"\"),\n",
    "        \"doi\": row.get(\"doi\",\"\"),\n",
    "        \"openalex_id\": row.get(\"openalex_id\",\"\"),\n",
    "        \"oa_url\": row.get(\"oa_url\",\"\"),\n",
    "        \"publisher\": row.get(\"host_venue.publisher\",\"\") if \"host_venue.publisher\" in row else row.get(\"publisher\",\"\"),\n",
    "        \"oa_status\": row.get(\"oa_status\",\"\"),\n",
    "        \"year\": row.get(\"year\",\"\"),\n",
    "    }\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(fields, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ======= LOAD INPUT CSV =======\n",
    "latest_csv = _find_latest_citing_csv(DATA_DIR, POINTER)\n",
    "csv_path = str(latest_csv)\n",
    "print(f\"[AUTO] Using latest citing CSV: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# sanitize key columns to avoid 'nan' or \"None\" requests\n",
    "df[\"doi\"] = df[\"doi\"].map(safe_str)\n",
    "df[\"oa_url\"] = df[\"oa_url\"].map(safe_str)\n",
    "df[\"openalex_id\"] = df[\"openalex_id\"].map(safe_str)\n",
    "\n",
    "# Candidates: (OA AND has oa_url) OR has a DOI (Unpaywall/OpenAlex repo may still work)\n",
    "is_oa_bool = df[\"is_oa\"].astype(str).str.lower().isin({\"true\", \"1\", \"yes\"}) | (df[\"is_oa\"] == True)\n",
    "oa_url_valid = df[\"oa_url\"].astype(str).str.strip().ne(\"\") & df[\"oa_url\"].notna()\n",
    "doi_ok = df[\"doi\"].astype(str).str.strip().ne(\"\") & df[\"doi\"].notna()\n",
    "\n",
    "candidates = df[(is_oa_bool & oa_url_valid) | doi_ok].copy()\n",
    "oa_url_hits = int(((is_oa_bool & oa_url_valid)).sum())\n",
    "doi_only_hits = int((doi_ok & ~oa_url_valid).sum())\n",
    "print(f\"Rows in CSV: {len(df)} | candidates: {len(candidates)}  (OA+URL={oa_url_hits}, DOI-only={doi_only_hits})\")\n",
    "\n",
    "# ======= OUTPUT DIR =======\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"downloaded_files_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ======= MAIN DOWNLOAD FUNCTION =======\n",
    "def download_and_save_file(row: pd.Series):\n",
    "    url = safe_str(row.get(\"oa_url\"))\n",
    "    title = safe_str(row.get(\"title\")) or \"\"\n",
    "    openalex_id = safe_str(row.get(\"openalex_id\")) or \"\"\n",
    "    # handle full OpenAlex URLs\n",
    "    if openalex_id and \"/\" in openalex_id:\n",
    "        openalex_id = openalex_id.split(\"/\")[-1]\n",
    "    filename = sanitize_filename(title or openalex_id or \"paper\")\n",
    "    dest_pdf = os.path.join(output_dir, f\"{filename}.pdf\")\n",
    "\n",
    "    # Skip if a valid PDF already exists\n",
    "    if os.path.exists(dest_pdf) and is_valid_pdf(dest_pdf):\n",
    "        # print(f\"[SKIP] Already downloaded: {filename}\")\n",
    "        return\n",
    "\n",
    "    doi = normalize_doi(safe_str(row.get(\"doi\")))\n",
    "\n",
    "    # 0) Domain-friendly pacing (if we have a URL)\n",
    "    if url:\n",
    "        domain_sleep(url)\n",
    "\n",
    "    # 1) Unpaywall first for strict publishers or when URL missing\n",
    "    tried_unpaywall_early = False\n",
    "    if doi and (prefer_unpaywall_first(url) or not url):\n",
    "        tried_unpaywall_early = True\n",
    "        for cand in get_unpaywall_candidate_pdfs(SESSION, doi):\n",
    "            domain_sleep(cand)\n",
    "            try:\n",
    "                rpdf = SESSION.get(cand, timeout=30, stream=True, allow_redirects=True)\n",
    "                if save_and_validate(rpdf, dest_pdf):\n",
    "                    print(f\"[UNPAYWALL LOCATION SAVED] {dest_pdf}\")\n",
    "                    return\n",
    "                if \"html\" in (rpdf.headers.get(\"Content-Type\") or \"\").lower():\n",
    "                    links = _resolve_pdf_links_from_html(rpdf.text, rpdf.url)\n",
    "                    for l in links:\n",
    "                        domain_sleep(l)\n",
    "                        try:\n",
    "                            if _download_pdf_with_headers(SESSION, l, dest_pdf, referer=rpdf.url):\n",
    "                                print(f\"[UNPAYWALL LANDING → PDF SAVED] {dest_pdf}\")\n",
    "                                return\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) Try OA URL directly (with MDPI fix and strong Referer)\n",
    "    if url:\n",
    "        fixed = mdpi_fix(url)\n",
    "        try:\n",
    "            hdrs = {\"Accept\":\"application/pdf\"}\n",
    "            # set a referer if domain might care\n",
    "            try_host = urlparse(fixed).hostname or \"\"\n",
    "            if \"mdpi.com\" in try_host:\n",
    "                hdrs[\"Referer\"] = \"https://www.mdpi.com/\"\n",
    "            r = SESSION.get(fixed, headers=hdrs, timeout=25, allow_redirects=True, stream=True)\n",
    "            if save_and_validate(r, dest_pdf):\n",
    "                print(f\"[PDF SAVED] {dest_pdf}\")\n",
    "                return\n",
    "            # if HTML, scrape embedded PDF links\n",
    "            if \"html\" in (r.headers.get(\"Content-Type\") or \"\").lower():\n",
    "                soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if (\".pdf\" in href) or (\"format=pdf\" in href) or href.rstrip(\"/\").endswith(\"/pdf\"):\n",
    "                        pdf_url = urljoin(r.url, href)\n",
    "                        domain_sleep(pdf_url)\n",
    "                        try:\n",
    "                            if _download_pdf_with_headers(SESSION, pdf_url, dest_pdf, referer=r.url):\n",
    "                                print(f\"[PDF FOUND IN HTML & SAVED] {dest_pdf}\")\n",
    "                                return\n",
    "                        except Exception:\n",
    "                            pass\n",
    "        except Exception as e:\n",
    "            print(f\"[DOWNLOAD ERROR] {fixed} — {e}\")\n",
    "\n",
    "    # 3) Wiley EPDF viewer fallback (if Wiley)\n",
    "    if doi and url:\n",
    "        host = urlparse(url).hostname or \"\"\n",
    "        if \"onlinelibrary.wiley.com\" in host:\n",
    "            if try_wiley_epdf(SESSION, doi, dest_pdf):\n",
    "                return\n",
    "\n",
    "    # 4) DOI fallback (content-negotiation + landing scrape + publisher patterns)\n",
    "    if doi:\n",
    "        try:\n",
    "            if download_pdf_from_doi(SESSION, doi, dest_pdf):\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f\"[DOI FALLBACK ERROR] {doi} — {e}\")\n",
    "\n",
    "    # 5) Unpaywall (if not tried earlier)\n",
    "    if doi and not tried_unpaywall_early:\n",
    "        for cand in get_unpaywall_candidate_pdfs(SESSION, doi):\n",
    "            domain_sleep(cand)\n",
    "            try:\n",
    "                rpdf = SESSION.get(cand, timeout=30, stream=True, allow_redirects=True)\n",
    "                if save_and_validate(rpdf, dest_pdf):\n",
    "                    print(f\"[UNPAYWALL LOCATION SAVED] {dest_pdf}\")\n",
    "                    return\n",
    "                if \"html\" in (rpdf.headers.get(\"Content-Type\") or \"\").lower():\n",
    "                    links = _resolve_pdf_links_from_html(rpdf.text, rpdf.url)\n",
    "                    for l in links:\n",
    "                        domain_sleep(l)\n",
    "                        try:\n",
    "                            if _download_pdf_with_headers(SESSION, l, dest_pdf, referer=rpdf.url):\n",
    "                                print(f\"[UNPAYWALL LANDING → PDF SAVED] {dest_pdf}\")\n",
    "                                return\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 6) OpenAlex repository pdf_url(s) (e.g., institutional repositories).\n",
    "    for cand in get_openalex_repository_pdfs(SESSION, openalex_id):\n",
    "        domain_sleep(cand)\n",
    "        try:\n",
    "            rpdf = SESSION.get(cand, timeout=30, stream=True, allow_redirects=True)\n",
    "            if save_and_validate(rpdf, dest_pdf):\n",
    "                print(f\"[OPENALEX REPO PDF SAVED] {dest_pdf}\")\n",
    "                return\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 7) Give up → log for manual\n",
    "    log_manual_download(row, output_dir)\n",
    "\n",
    "# ======= DOWNLOAD LOOP (demo-cap aware) =======\n",
    "iter_df = candidates.head(DEMO_MAX) if DEMO else candidates\n",
    "for _, row in iter_df.iterrows():\n",
    "    download_and_save_file(row)\n",
    "\n",
    "print(f\"\\n Done downloading files to: {output_dir}\")\n",
    "\n",
    "# ======= SUMMARY & POINTER FOR STEP 3 =======\n",
    "saved = len([f for f in os.listdir(output_dir) if f.lower().endswith(\".pdf\") and is_valid_pdf(os.path.join(output_dir,f))])\n",
    "manual_log = os.path.join(output_dir, \"manual_download_required.txt\")\n",
    "manual = sum(1 for _ in open(manual_log, \"r\", encoding=\"utf-8\")) if os.path.exists(manual_log) else 0\n",
    "print(f\"Summary: PDFs saved = {saved}, manual downloads logged = {manual}\")\n",
    "\n",
    "(DATA_DIR / \"latest_download_dir.txt\").write_text(str(Path(output_dir).resolve()), encoding=\"utf-8\")\n",
    "\n",
    "# ======= OPTIONAL: MANUAL UPLOADS PROCESSING =======\n",
    "manual_uploads_dir = \"manual_uploads\"  # drop files manually here\n",
    "os.makedirs(manual_uploads_dir, exist_ok=True)\n",
    "\n",
    "def process_manual_uploads():\n",
    "    print(\"\\n=== Processing manual uploads ===\")\n",
    "    for filename in os.listdir(manual_uploads_dir):\n",
    "        file_path = os.path.join(manual_uploads_dir, filename)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        out_pdf = os.path.join(output_dir, f\"{name}.pdf\")\n",
    "        out_txt = os.path.join(output_dir, f\"{name}.txt\")\n",
    "\n",
    "        if ext.lower() == \".pdf\":\n",
    "            if not os.path.exists(out_pdf):\n",
    "                shutil.copy(file_path, out_pdf)\n",
    "                print(f\"[PDF COPIED] {filename}\")\n",
    "            else:\n",
    "                print(f\"[SKIP] Exists: {filename}\")\n",
    "        elif ext.lower() == \".txt\":\n",
    "            if not os.path.exists(out_txt):\n",
    "                shutil.copy(file_path, out_txt)\n",
    "                print(f\"[TEXT COPIED] {filename}\")\n",
    "            else:\n",
    "                print(f\"[SKIP] Exists: {filename}\")\n",
    "        elif ext.lower() == \".html\":\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    html = f.read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                text = \"\\n\".join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "                with open(out_txt, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                    f_out.write(text.strip())\n",
    "                print(f\"[HTML TEXT EXTRACTED] {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] HTML extract failed {filename} — {e}\")\n",
    "        else:\n",
    "            print(f\"[IGNORED] Unsupported type: {filename}\")\n",
    "\n",
    "process_manual_uploads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c37b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200 | Body: true\n"
     ]
    }
   ],
   "source": [
    "# Health check: GROBID should return HTTP 200\n",
    "import requests\n",
    "GROBID_URL = \"http://localhost:8070\"\n",
    "r = requests.get(f\"{GROBID_URL}/api/isalive\", timeout=5)\n",
    "print(\"Status:\", r.status_code, \"| Body:\", r.text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbc6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] Using PDF dir from pointer: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\n",
      "[OUT] TEI dir: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\tei\n",
      "GROBID alive: true\n",
      "[AUTO] Loaded TARGET: An Isometry Classification of Periodic Point Sets  (DOI=10.1007/978-3-030-76657-3_16)\n",
      "PDFs found: 25\n",
      " - A continuous map of 2.6+ million 2D lattices from the Cambridge Structural Database.pdf\n",
      " - A unique and continuous code of all periodic crystals.pdf\n",
      " - Analogy Powered by Prediction and Structural Invariants_ Computationally Led Discovery of a Mesoporous Hydrogen-Bonded O.pdf\n",
      " - Average minimum distances of periodic point sets – foundational invariants for mapping periodic crystals.pdf\n",
      " - Bounds for the Regularity Radius of Delone Sets.pdf\n",
      " - Continuous chiral distances for two‐dimensional lattices.pdf\n",
      " - Continuous Invariant-Based Maps of the Cambridge Structural Database.pdf\n",
      " - Continuous maps of molecules and atomic clouds in large databases.pdf\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG: PDF → TEI pipeline (auto-pick latest; pointer-aware) ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "# Paths/pointers\n",
    "DATA_DIR = Path(r\"C:\\Users\\Karthik\\Desktop\\MSc Project\\data\")\n",
    "POINTER_DL = DATA_DIR / \"latest_download_dir.txt\"   # written by Step 2\n",
    "\n",
    "# 1) Find the PDF folder created in Cell  2 (using a pointer file; otherwise guessing sensibly).\n",
    "PDF_DIR = None\n",
    "if POINTER_DL.exists():\n",
    "    try:\n",
    "        hinted = Path(POINTER_DL.read_text(encoding=\"utf-8\").strip())\n",
    "        if hinted.exists():\n",
    "            PDF_DIR = hinted\n",
    "            print(f\"[AUTO] Using PDF dir from pointer: {PDF_DIR}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if PDF_DIR is None:\n",
    "    candidates = sorted(Path.cwd().glob(\"downloaded_files_*\"),\n",
    "                        key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not candidates:\n",
    "        vs_code_dir = Path(r\"C:\\Users\\Karthik\\Documents\\VS Code\")\n",
    "        if vs_code_dir.exists():\n",
    "            candidates = sorted(vs_code_dir.glob(\"downloaded_files_*\"),\n",
    "                                key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    PDF_DIR = candidates[0] if candidates else Path(r\"C:\\Users\\Karthik\\Documents\\VS Code\\downloaded_files_2025-08-18_03-04-47\")\n",
    "    print(f\"[AUTO] Using PDF dir (glob/fallback): {PDF_DIR}\")\n",
    "\n",
    "assert PDF_DIR.exists(), f\"PDF_DIR not found: {PDF_DIR}\"\n",
    "\n",
    "# 2) Outputs (Create output folder, timestamped to avoid overwriting) + pointer for Step 4\n",
    "OUT_DIR = PDF_DIR / f\"grobid_outputs_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "TEI_DIR = OUT_DIR / \"tei\"\n",
    "TEI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[OUT] TEI dir: {TEI_DIR}\")\n",
    "(DATA_DIR / \"latest_tei_dir.txt\").write_text(str(TEI_DIR.resolve()), encoding=\"utf-8\")\n",
    "\n",
    "# 3) GROBID server health check\n",
    "GROBID_URL = \"http://localhost:8070\"\n",
    "try:\n",
    "    alive = requests.get(f\"{GROBID_URL}/api/isalive\", timeout=5).text.strip().lower()\n",
    "    print(\"GROBID alive:\", alive)\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"GROBID not reachable at {GROBID_URL}. Start it, then re-run. ({e})\")\n",
    "\n",
    "# 4) Auto-load TARGET from Step 1 (with fallback)\n",
    "target_path = DATA_DIR / \"target_work.json\"\n",
    "if target_path.exists():\n",
    "    T = json.loads(target_path.read_text(encoding=\"utf-8\"))\n",
    "    TARGET = {\n",
    "        \"doi\": T.get(\"doi\"),\n",
    "        \"title\": T.get(\"title\"),\n",
    "        \"authors\": [T.get(\"first_author\",\"\")],\n",
    "        \"year\": T.get(\"year\"),\n",
    "    }\n",
    "    print(f\"[AUTO] Loaded TARGET: {TARGET['title']}  (DOI={TARGET['doi']})\")\n",
    "else:\n",
    "    TARGET = {\n",
    "        \"doi\": \"10.1007/978-3-319-08434-3_7\",\n",
    "        \"title\": \"Detecting unknots via equational reasoning, I: Exploration\",\n",
    "        \"authors\": [\"Andrew Fish\", \"Alexei Lisitsa\"],\n",
    "        \"year\": 2014,\n",
    "    }\n",
    "    print(\"[FALLBACK TARGET] Using hardcoded Lisitsa paper.\")\n",
    "\n",
    "# 5) Quick preview of PDFs detected\n",
    "pdfs = sorted([p for p in PDF_DIR.glob(\"*.pdf\") if p.stat().st_size > 0])\n",
    "print(f\"PDFs found: {len(pdfs)}\")\n",
    "if not pdfs:\n",
    "    print(\" No PDFs in this folder. Did Step 2 finish? Check pointer or directory.\")\n",
    "for p in pdfs[:8]:\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d843c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 25 files\n",
      "[OK]   A continuous map of 2.6+ million 2D lattices from the Cambridge Structural Database.pdf → A continuous map of 2.6+ million 2D lattices from the Cambridge Structural Database.tei.xml\n",
      "[OK]   A unique and continuous code of all periodic crystals.pdf → A unique and continuous code of all periodic crystals.tei.xml\n",
      "[OK]   Analogy Powered by Prediction and Structural Invariants_ Computationally Led Discovery of a Mesoporous Hydrogen-Bonded O.pdf → Analogy Powered by Prediction and Structural Invariants_ Computationally Led Discovery of a Mesoporous Hydrogen-Bonded O.tei.xml\n",
      "[OK]   Average minimum distances of periodic point sets – foundational invariants for mapping periodic crystals.pdf → Average minimum distances of periodic point sets – foundational invariants for mapping periodic crystals.tei.xml\n",
      "[OK]   Bounds for the Regularity Radius of Delone Sets.pdf → Bounds for the Regularity Radius of Delone Sets.tei.xml\n",
      "[OK]   Continuous chiral distances for two‐dimensional lattices.pdf → Continuous chiral distances for two‐dimensional lattices.tei.xml\n",
      "[OK]   Continuous Invariant-Based Maps of the Cambridge Structural Database.pdf → Continuous Invariant-Based Maps of the Cambridge Structural Database.tei.xml\n",
      "[OK]   Continuous maps of molecules and atomic clouds in large databases.pdf → Continuous maps of molecules and atomic clouds in large databases.tei.xml\n",
      "[OK]   Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006.pdf → Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006.tei.xml\n",
      "[OK]   Density Functions of Periodic Sequences of Continuous Events.pdf → Density Functions of Periodic Sequences of Continuous Events.tei.xml\n",
      "[OK]   Entropic Trust Region for Densest Crystallographic Symmetry Group Packings.pdf → Entropic Trust Region for Densest Crystallographic Symmetry Group Packings.tei.xml\n",
      "[OK]   Families of point sets with identical 1D persistence.pdf → Families of point sets with identical 1D persistence.tei.xml\n",
      "[OK]   Fast Predictions of Lattice Energies by Continuous Isometry Invariants of Crystal Structures.pdf → Fast Predictions of Lattice Energies by Continuous Isometry Invariants of Crystal Structures.tei.xml\n",
      "[OK]   Generic families of finite metric spaces with identical or trivial 1-dimensional persistence.pdf → Generic families of finite metric spaces with identical or trivial 1-dimensional persistence.tei.xml\n",
      "[OK]   Geographic style maps for two-dimensional lattices.pdf → Geographic style maps for two-dimensional lattices.tei.xml\n",
      "[OK]   Geographic-style maps with a local novelty distance help navigate in the materials space.pdf → Geographic-style maps with a local novelty distance help navigate in the materials space.tei.xml\n",
      "[OK]   Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances.pdf → Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances.tei.xml\n",
      "[OK]   Introduction to invariant-based machine learning for periodic crystals.pdf → Introduction to invariant-based machine learning for periodic crystals.tei.xml\n",
      "[OK]   Material Property Prediction Using Graphs Based on Generically Complete Isometry Invariants.pdf → Material Property Prediction Using Graphs Based on Generically Complete Isometry Invariants.tei.xml\n",
      "[OK]   Mathematics of 2-Dimensional Lattices.pdf → Mathematics of 2-Dimensional Lattices.tei.xml\n",
      "[OK]   Polynomial-Time Algorithms for Continuous Metrics on Atomic Clouds of Unordered Points.pdf → Polynomial-Time Algorithms for Continuous Metrics on Atomic Clouds of Unordered Points.tei.xml\n",
      "[OK]   Potential materials genome for mapping the continuous space of all periodic crystals.pdf → Potential materials genome for mapping the continuous space of all periodic crystals.tei.xml\n",
      "[OK]   Recognition of Near-Duplicate Periodic Patterns by Continuous Metrics with Approximation Guarantees.pdf → Recognition of Near-Duplicate Periodic Patterns by Continuous Metrics with Approximation Guarantees.tei.xml\n",
      "[OK]   Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negati.pdf → Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negati.tei.xml\n",
      "[OK]   The importance of definitions in crystallography.pdf → The importance of definitions in crystallography.tei.xml\n",
      "\n",
      "TEI done. ok=25, failed=0. Out: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\tei\n"
     ]
    }
   ],
   "source": [
    "# === PDFs → TEI via GROBID ===\n",
    "# This cell converts *all* detected PDFs into TEI XML using a locally running GROBID server.\n",
    "# It assumes the previous cell already defined:\n",
    "#   - `pdfs`   : a list of Path objects pointing to PDFs to process\n",
    "#   - `TEI_DIR`: a Path to the output directory where TEI files should be written\n",
    "#   - `GROBID_URL`: the base URL of your local GROBID service, e.g., \"http://localhost:8070\"\n",
    "#\n",
    "# Flow:\n",
    "#   1) Prepare a per-file converter function `pdf_to_tei(...)`.\n",
    "#   2) Build the list of PDFs to run (all non-empty PDFs).\n",
    "#   3) Convert each PDF → TEI, skipping ones already converted.\n",
    "#   4) Print a summary and log failures to a text file.\n",
    "\n",
    "import time, requests\n",
    "from pathlib import Path\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Process all PDFs; set to an integer only if ever want to cap the count.\n",
    "# Keeping a small pause between files avoids overwhelming GROBID on large batches.\n",
    "MAX_PDFS = None\n",
    "PER_FILE_PAUSE = 0.3  # keep a short pause so GROBID isn't hammered; set 0 if you want\n",
    "\n",
    "def pdf_to_tei(\n",
    "    pdf_path: Path,\n",
    "    out_path: Path,\n",
    "    base_url: str = GROBID_URL,\n",
    "    consolidate_citations: int = 1,\n",
    "    retry: int = 2,\n",
    "    sleep_s: float = 1.5,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Convert a single PDF to TEI using a running GROBID service.\n",
    "\n",
    "    This function sends the PDF to GROBID's /api/processFulltextDocument endpoint and,\n",
    "    on success, writes the returned TEI XML to `out_path`. It includes basic robustness:\n",
    "    - small retry loop on network or non-5xx issues,\n",
    "    - early bail-out on 5xx (server) errors,\n",
    "    - simple validation that the response contains a <TEI tag.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (Path): Absolute or relative path to the input PDF file.\n",
    "        out_path (Path): Path where the resulting TEI XML should be saved.\n",
    "        base_url (str): Base URL of the GROBID server (e.g., \"http://localhost:8070\").\n",
    "        consolidate_citations (int): GROBID option for citation normalization (1 is a solid default).\n",
    "        retry (int): Number of retries after the initial attempt (total attempts = retry + 1).\n",
    "        sleep_s (float): Base seconds to sleep between retries. Increases slightly each attempt.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if TEI was received (HTTP 200 and contains '<TEI') and saved; False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    # Fulltext conversion endpoint exposed by GROBID\n",
    "    url = f\"{base_url}/api/processFulltextDocument\"\n",
    "\n",
    "    # Attempt the request up to (retry + 1) times\n",
    "    for attempt in range(retry + 1):\n",
    "        try:\n",
    "            # Send the PDF as multipart form-data with expected field name \"input\"\n",
    "            with open(pdf_path, \"rb\") as f:\n",
    "                files = {\"input\": (pdf_path.name, f, \"application/pdf\")}\n",
    "                data = {\n",
    "                    \"consolidateCitations\": consolidate_citations,\n",
    "                    \"includeRawCitations\": 0,\n",
    "                    \"includeRawAffiliations\": 0,\n",
    "                    \"teiCoordinates\": \"ref,biblStruct\",\n",
    "                }\n",
    "                # Generous timeout for large or complex PDFs\n",
    "                r = requests.post(url, files=files, data=data, timeout=120)\n",
    "\n",
    "            # Read response body as text (TEI is XML text). Strip BOM/leading whitespace if present.\n",
    "            txt = (r.text or \"\")\n",
    "            txt_l = txt.lstrip(\"\\ufeff \\t\\r\\n\")  # trim BOM/whitespace\n",
    "\n",
    "            # Minimal check: valid TEI should contain a '<TEI' element somewhere.\n",
    "            # (We don't enforce XML prolog since some servers omit it.)\n",
    "            looks_like_tei = (\"<TEI\" in txt_l)\n",
    "\n",
    "            # Success path: HTTP 200 + TEI detected → write to disk and return True\n",
    "            if r.status_code == 200 and looks_like_tei:\n",
    "                out_path.write_text(txt, encoding=\"utf-8\")\n",
    "                return True\n",
    "\n",
    "            # Prepare a short snippet for logging/warnings (first ~140 chars, single line)\n",
    "            snippet = txt_l[:140].replace(\"\\n\", \" \")\n",
    "\n",
    "            # For 5xx server errors, retries rarely help; log a warning and break out early.\n",
    "            if r.status_code >= 500:\n",
    "                print(f\"[WARN] {pdf_path.name}: HTTP {r.status_code} — {snippet}…\")\n",
    "                break\n",
    "\n",
    "            # For other statuses (e.g., 4xx or odd HTML), warn and allow the retry loop to continue.\n",
    "            print(f\"[WARN] {pdf_path.name}: HTTP {r.status_code} — {snippet}…\")\n",
    "\n",
    "        except RequestException as e:\n",
    "            # Network-level errors (timeouts, connection issues) get logged; we may retry.\n",
    "            print(f\"[ERR]  {pdf_path.name}: {e}\")\n",
    "\n",
    "        # Backoff before the next attempt (if any remain). Increases a bit each time.\n",
    "        if attempt < retry:\n",
    "            time.sleep(sleep_s * (1 + attempt))\n",
    "\n",
    "    # If we exit the loop without returning True, the conversion failed.\n",
    "    return False\n",
    "\n",
    "# Build the list of PDFs to run. We skip zero-byte files (they're not real PDFs).\n",
    "# With MAX_PDFS=None we do NOT apply any cap: all PDFs will be considered.\n",
    "pdfs_to_run = [p for p in pdfs if p.stat().st_size > 0]\n",
    "print(\"Converting:\", len(pdfs_to_run), \"files\")\n",
    "\n",
    "ok = bad = 0\n",
    "fails = []\n",
    "\n",
    "# Process each PDF → TEI.\n",
    "for pdf in pdfs_to_run:\n",
    "    # Output TEI path mirrors the PDF name but ends with \".tei.xml\"\n",
    "    tei_path = TEI_DIR / (pdf.stem + \".tei.xml\")\n",
    "\n",
    "    # If TEI already exists and is non-empty, skip to avoid rework on re-runs.\n",
    "    if tei_path.exists() and tei_path.stat().st_size > 0:\n",
    "        print(f\"[SKIP] {tei_path.name}\")\n",
    "        ok += 1\n",
    "        continue\n",
    "\n",
    "    # Attempt conversion and log outcome.\n",
    "    if pdf_to_tei(pdf, tei_path):\n",
    "        print(f\"[OK]   {pdf.name} → {tei_path.name}\")\n",
    "        ok += 1\n",
    "    else:\n",
    "        print(f\"[FAIL] {pdf.name}\")\n",
    "        bad += 1\n",
    "        fails.append(pdf.name)\n",
    "\n",
    "    # Small pause between files to keep GROBID responsive.\n",
    "    time.sleep(PER_FILE_PAUSE)\n",
    "\n",
    "# Print a concise summary and write failures (if any) to a text file for easy re-tries later.\n",
    "print(f\"\\nTEI done. ok={ok}, failed={bad}. Out: {TEI_DIR}\")\n",
    "if fails:\n",
    "    fail_log = TEI_DIR.parent / \"grobid_failures.txt\"\n",
    "    fail_log.write_text(\"\\n\".join(fails), encoding=\"utf-8\")\n",
    "    print(f\"[LOG] Failure list → {fail_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c55b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI files: 25\n",
      " Robust TEI counts saved:\n",
      "- CSV: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\tei_counts_robust_2025-09-13_08-58-28.csv\n",
      "- JSONL: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\tei_contexts_robust_2025-09-13_08-58-28.jsonl\n",
      "- Debug: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\tei_match_debug_2025-09-13_08-58-28.csv\n",
      "Files: 25\n"
     ]
    }
   ],
   "source": [
    "# === TEI robust matcher + counter (regex targets; any ref/ptr; arXiv-in-text; authors/year; xml:id fix) ===\n",
    "# This cell parses GROBID TEI XML files, matches the correct target reference,\n",
    "# counts in-text citations, and saves outputs (counts, contexts, debug info).\n",
    "\n",
    "from pathlib import Path\n",
    "from xml.etree import ElementTree as ET\n",
    "import re, csv, json, difflib\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Preconditions ---\n",
    "assert 'TEI_DIR' in globals() and TEI_DIR.exists(), \"TEI_DIR missing. Run the PDFs→TEI step first.\"\n",
    "assert 'OUT_DIR' in globals() and OUT_DIR.exists(), \"OUT_DIR missing. Run the CONFIG cell first.\"\n",
    "assert 'TARGET' in globals(), \"TARGET not defined.\"\n",
    "\n",
    "XML_ID = \"{http://www.w3.org/XML/1998/namespace}id\"  # fully-qualified xml:id\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalize a string: lowercase, collapse whitespace.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").lower()).strip()\n",
    "\n",
    "def _fuzzy(a: str, b: str) -> float:\n",
    "    \"\"\"Returns similarity score between two strings (0 to 1). Used for title matching..\"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def _get_text(el) -> str:\n",
    "    \"\"\"Safely pull all visible text from an XML node (including nested)..\"\"\"\n",
    "    return \" \".join(el.itertext()).strip() if el is not None else \"\"\n",
    "\n",
    "# extract all #bXXX ids from a @target string (handles spaces, commas, semicolons, ranges)\n",
    "_TARGET_ID_RE = re.compile(r\"#([A-Za-z0-9_-]+)\")\n",
    "\n",
    "def extract_target_ids(targ_attr: str):\n",
    "    #In TEI, in-text citations use target=\"#b12\" (or multiple IDs like \"#b3 #b12\").\n",
    "    # This finds all IDs referenced inside a target attribute and returns them as a list (without the #).\n",
    "    \n",
    "    if not targ_attr:\n",
    "        return []\n",
    "    return _TARGET_ID_RE.findall(targ_attr)\n",
    "\n",
    "def collect_bibl_structs(root: ET.Element):\n",
    "    \"\"\"\n",
    "    Collect all <biblStruct> nodes (the references).\n",
    "    Returns (list_of_bibls, set_of_all_ids).\n",
    "    Each bibl dict contains: id, title, authors, year, idnos, text.\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    all_ids = set()\n",
    "    for b in root.findall(\".//{*}biblStruct\"):\n",
    "        bid = b.get(XML_ID)\n",
    "        all_ids.add(bid)\n",
    "\n",
    "        # Title (prefer analytic/monogr titles)\n",
    "        tnode = (b.find(\".//{*}analytic/{*}title\") or\n",
    "                 b.find(\".//{*}monogr/{*}title\") or\n",
    "                 b.find(\".//{*}title\"))\n",
    "        title = _get_text(tnode)\n",
    "\n",
    "        # Authors (surnames only)\n",
    "        authors = []\n",
    "        for pers in b.findall(\".//{*}author\"):\n",
    "            sname = pers.find(\".//{*}surname\")\n",
    "            if sname is not None and (sname.text or \"\").strip():\n",
    "                authors.append(sname.text.strip())\n",
    "\n",
    "        # Year\n",
    "        year = None\n",
    "        d = b.find(\".//{*}date\")\n",
    "        if d is not None:\n",
    "            y = (d.get(\"when\") or d.get(\"notBefore\") or d.get(\"notAfter\") or \"\").strip()\n",
    "            m = re.search(r\"(19|20)\\d{2}\", y)\n",
    "            if m:\n",
    "                year = int(m.group(0))\n",
    "        if year is None:\n",
    "            m = re.search(r\"(19|20)\\d{2}\", _get_text(b))\n",
    "            if m:\n",
    "                year = int(m.group(0))\n",
    "\n",
    "        # idnos (DOI, arXiv, URL, etc.)\n",
    "        idnos = {}\n",
    "        for idno in b.findall(\".//{*}idno\"):\n",
    "            idtype = (idno.get(\"type\") or \"\").lower()\n",
    "            idnos.setdefault(idtype, set()).add(_norm(idno.text))\n",
    "\n",
    "        out.append({\n",
    "            \"id\": bid,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"idnos\": idnos,\n",
    "            \"text\": _get_text(b)\n",
    "        })\n",
    "    return out, all_ids\n",
    "\n",
    "def _find_bnode_by_id(root: ET.Element, bid: str):\n",
    "    \"\"\"Find the <biblStruct> with a given xml:id. \n",
    "    Utility for scanning inside a known reference node (used for arXiv pointer checks).\"\"\"\n",
    "    \n",
    "    if not bid:\n",
    "        return None\n",
    "    for cand in root.findall(\".//{*}biblStruct\"):\n",
    "        if cand.get(XML_ID) == bid:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "def score_bibl(bibl: dict, target: dict, root_for_attr_scan=None):\n",
    "    \"\"\"\n",
    "    Multi-cue score:\n",
    "      - DOI exact (+1.0)\n",
    "      - arXiv ID present (+0.9)\n",
    "      - fuzzy title (+ratio)\n",
    "      - author surname overlap (+0.15 each)\n",
    "      - year match (+0.1)\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    reasons = []\n",
    "\n",
    "    # DOI exact\n",
    "    tgt_doi = _norm(target.get(\"doi\", \"\"))\n",
    "    if tgt_doi and \"doi\" in bibl[\"idnos\"] and tgt_doi in bibl[\"idnos\"][\"doi\"]:\n",
    "        score += 1.0; reasons.append(\"doi\")\n",
    "\n",
    "    # arXiv id detection\n",
    "    arx = None\n",
    "    if tgt_doi:\n",
    "        m = re.search(r\"arxiv\\.(\\d{4}\\.\\d{4,5})\", tgt_doi, flags=re.I)\n",
    "        arx = m.group(1).lower() if m else None\n",
    "\n",
    "    if arx:\n",
    "        arx_norm = arx\n",
    "        has_arxiv_idno = (\"arxiv\" in bibl[\"idnos\"] and any(arx_norm in x for x in bibl[\"idnos\"][\"arxiv\"]))\n",
    "        has_arxiv_url  = (\"url\" in bibl[\"idnos\"]   and any(arx_norm in x for x in bibl[\"idnos\"][\"url\"]))\n",
    "        has_arxiv_ptr  = False\n",
    "        if root_for_attr_scan is not None and bibl[\"id\"]:\n",
    "            bnode = _find_bnode_by_id(root_for_attr_scan, bibl[\"id\"])\n",
    "            if bnode is not None:\n",
    "                for el in bnode.findall(\".//{*}ptr\"):\n",
    "                    tgt = _norm(el.get(\"target\", \"\") or \"\")\n",
    "                    if arx_norm in tgt: has_arxiv_ptr = True; break\n",
    "                if not has_arxiv_ptr:\n",
    "                    for el in bnode.findall(\".//{*}ref\"):\n",
    "                        tgt = _norm(el.get(\"target\", \"\") or \"\")\n",
    "                        if arx_norm in tgt: has_arxiv_ptr = True; break\n",
    "        has_arxiv_text = bool(re.search(re.escape(arx_norm), _norm(bibl[\"text\"])))\n",
    "        if has_arxiv_idno or has_arxiv_url or has_arxiv_ptr or has_arxiv_text:\n",
    "            score += 0.9; reasons.append(\"arxiv_id\")\n",
    "\n",
    "    # fuzzy title\n",
    "    tgt_title = _norm(target.get(\"title\", \"\"))\n",
    "    if tgt_title and bibl[\"title\"]:\n",
    "        r = _fuzzy(tgt_title, _norm(bibl[\"title\"]))\n",
    "        if r >= 0.40:\n",
    "            score += r; reasons.append(f\"title_fuzzy:{r:.3f}\")\n",
    "\n",
    "    # author surnames\n",
    "    tgt_authors = [a.lower() for a in (target.get(\"authors\") or [])]\n",
    "    if tgt_authors and bibl[\"authors\"]:\n",
    "        overlap = len(set(tgt_authors) & set(a.lower() for a in bibl[\"authors\"]))\n",
    "        if overlap:\n",
    "            score += 0.15 * overlap; reasons.append(f\"authors:{overlap}\")\n",
    "\n",
    "    # year\n",
    "    if target.get(\"year\") and bibl[\"year\"] and int(target[\"year\"]) == int(bibl[\"year\"]):\n",
    "        score += 0.1; reasons.append(\"year\")\n",
    "\n",
    "    return score, reasons\n",
    "\n",
    "# --------- Block helpers ---------\n",
    "_BLOCK_TAGS = {\"p\",\"head\",\"note\",\"figure\",\"figDesc\",\"table\",\"cell\",\"item\",\"caption\",\"s\",\"div\"}\n",
    "\n",
    "def build_parent_map(root: ET.Element):\n",
    "    \"\"\"Map child->parent for all nodes.\"\"\"\n",
    "    mp = {}\n",
    "    for parent in root.iter():\n",
    "        for child in list(parent):\n",
    "            mp[child] = parent\n",
    "    return mp\n",
    "\n",
    "def climb_to_block(elem, parent_map):\n",
    "    \"\"\"Climb from element up to nearest block (paragraph, figure, table, etc.).\"\"\"\n",
    "    cur = elem\n",
    "    while cur is not None:\n",
    "        tag = cur.tag.split('}')[-1]\n",
    "        if tag in _BLOCK_TAGS:\n",
    "            return cur\n",
    "        cur = parent_map.get(cur)\n",
    "    return None\n",
    "\n",
    "# --- Math-index deflation (to avoid false counts from [1] in formulas) ---\n",
    "_NUMERIC_1 = re.compile(r\"^\\s*1\\s*$\")\n",
    "_LETTER_CLASS = r\"A-Za-z\\u0370-\\u03FF\"\n",
    "_STOPWORDS = {\"in\",\"as\",\"of\",\"by\",\"we\",\"on\",\"at\",\"to\",\"if\",\"or\",\"and\",\"for\",\"with\",\"from\",\n",
    "              \"see\",\"cf\",\"eg\",\"e.g\",\"via\",\"per\",\"vs\",\"et\",\"al\",\"fig\",\"fig.\",\"eq\",\"eq.\",\n",
    "              \"sec\",\"section\",\"theorem\",\"lemma\",\"prop\",\"proof\"}\n",
    "\n",
    "def _make_markerized_block_text(parent, ref_el):\n",
    "    \"\"\"Return block text with markers around a specific <ref> element.\"\"\"\n",
    "    parts = []\n",
    "    def walk(node):\n",
    "        if node is ref_el:\n",
    "            parts.append(\">>>REF<<<\")\n",
    "            parts.append(\"\".join(ref_el.itertext()) or \"\")\n",
    "            parts.append(\">>>/REF<<<\")\n",
    "        else:\n",
    "            if node.text:\n",
    "                parts.append(node.text)\n",
    "            for ch in list(node):\n",
    "                walk(ch)\n",
    "                if ch.tail:\n",
    "                    parts.append(ch.tail)\n",
    "    walk(parent)\n",
    "    return \"\".join(parts)\n",
    "\n",
    "def _looks_like_b0_math_index(ref_el, parent_block, bibl_id: str) -> bool:\n",
    "    \"\"\"Return True if ref=b0 with text '1' and preceded by variable-like token.\"\"\"\n",
    "    if bibl_id != \"b0\":\n",
    "        return False\n",
    "    ref_txt = (\"\".join(ref_el.itertext()) or \"\").strip()\n",
    "    if not _NUMERIC_1.match(ref_txt):\n",
    "        return False\n",
    "\n",
    "    s = _make_markerized_block_text(parent_block, ref_el)\n",
    "    s_norm = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    pat_out = re.compile(rf\"([{_LETTER_CLASS}]{{1,2}})\\s*\\[\\s*>>>REF<<<\\s*1\\s*>>>/REF<<<\\s*\\]\")\n",
    "    pat_in  = re.compile(rf\"([{_LETTER_CLASS}]{{1,2}})\\s*>>>REF<<<\\s*\\[\\s*1\\s*\\]\\s*>>>/REF<<<\")\n",
    "\n",
    "    m = pat_out.search(s_norm) or pat_in.search(s_norm)\n",
    "    if not m:\n",
    "        return False\n",
    "    token = (m.group(1) or \"\").lower()\n",
    "    if token in _STOPWORDS:\n",
    "        return False\n",
    "    if len(token) <= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def count_refs_and_contexts(root: ET.Element, bibl_id: str, valid_ids: set, max_ctx=20):\n",
    "    \"\"\"\n",
    "    Count all <ref>/<ptr> pointing to bibl_id.\n",
    "    Applies targeted math deflation for [1] false positives.\n",
    "    \"\"\"\n",
    "    if not bibl_id:\n",
    "        return 0, []\n",
    "    parent_map = build_parent_map(root)\n",
    "    kept, ctx = 0, []\n",
    "    for xp in (\".//{*}ref\", \".//{*}ptr\"):\n",
    "        for el in root.findall(xp):\n",
    "            ids = [i for i in extract_target_ids(el.get(\"target\",\"\") or \"\") if i in valid_ids]\n",
    "            if bibl_id in ids:\n",
    "                blk = climb_to_block(el, parent_map)\n",
    "                if blk is not None and _looks_like_b0_math_index(el, blk, bibl_id):\n",
    "                    continue\n",
    "                kept += 1\n",
    "                text = \" \".join(blk.itertext()).strip() if blk is not None else \" \".join(el.itertext()).strip()\n",
    "                if text:\n",
    "                    ctx.append(text)\n",
    "                    if len(ctx) >= max_ctx:\n",
    "                        return kept, ctx\n",
    "    return kept, ctx\n",
    "\n",
    "def quick_count(root: ET.Element, bid: str, valid_ids: set) -> int:\n",
    "    \"\"\"Fast count of <ref>/<ptr> for a given id (no contexts).\"\"\"\n",
    "    if not bid: return 0\n",
    "    n = 0\n",
    "    for xp in (\".//{*}ref\", \".//{*}ptr\"):\n",
    "        for el in root.findall(xp):\n",
    "            ids = [i for i in extract_target_ids(el.get(\"target\",\"\") or \"\") if i in valid_ids]\n",
    "            if bid in ids: n += 1\n",
    "    return n\n",
    "\n",
    "# ---------- Run ----------\n",
    "tei_files = sorted(TEI_DIR.glob(\"*.tei.xml\"))\n",
    "print(\"TEI files:\", len(tei_files))\n",
    "\n",
    "rows = []\n",
    "match_debug = []\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "csv_counts = OUT_DIR / f\"tei_counts_robust_{ts}.csv\"\n",
    "jsonl_ctx  = OUT_DIR / f\"tei_contexts_robust_{ts}.jsonl\"\n",
    "csv_dbg    = OUT_DIR / f\"tei_match_debug_{ts}.csv\"\n",
    "\n",
    "with open(jsonl_ctx, \"w\", encoding=\"utf-8\") as jout:\n",
    "    for tei in tei_files:\n",
    "        root = ET.parse(tei).getroot()\n",
    "        bibls, all_ids = collect_bibl_structs(root)\n",
    "\n",
    "        # score all candidates\n",
    "        scored = []\n",
    "        for b in bibls:\n",
    "            s, reasons = score_bibl(b, TARGET, root_for_attr_scan=root)\n",
    "            scored.append((s, reasons, b))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # debug: keep top 5 candidates\n",
    "        for rank, (s, reasons, b) in enumerate(scored[:5], start=1):\n",
    "            match_debug.append({\n",
    "                \"file\": tei.name, \"rank\": rank, \"bibl_id\": b[\"id\"] or \"\",\n",
    "                \"score\": f\"{s:.3f}\", \"reasons\": \";\".join(reasons),\n",
    "                \"title\": b[\"title\"], \"authors\": \", \".join(b[\"authors\"]),\n",
    "                \"year\": b[\"year\"] or \"\",\n",
    "                \"idnos_doi\": \", \".join(sorted(b[\"idnos\"].get(\"doi\", []))),\n",
    "                \"idnos_arxiv\": \", \".join(sorted(b[\"idnos\"].get(\"arxiv\", []))),\n",
    "                \"idnos_url\": \", \".join(sorted(b[\"idnos\"].get(\"url\", []))),\n",
    "            })\n",
    "\n",
    "        # select: top score, but fallback to one with actual in-text hits\n",
    "        topK = scored[:8]\n",
    "        if topK:\n",
    "            best_s, best_reasons, best_b = topK[0]\n",
    "            best_id = best_b[\"id\"]\n",
    "            best_count = quick_count(root, best_id, all_ids)\n",
    "\n",
    "            if best_count == 0:\n",
    "                for s, reasons, b in topK[1:]:\n",
    "                    c = quick_count(root, b[\"id\"], all_ids)\n",
    "                    if c > best_count or (c == best_count and s > best_s):\n",
    "                        best_s, best_reasons, best_b = s, reasons, b\n",
    "                        best_id, best_count = b[\"id\"], c\n",
    "\n",
    "            accept = (best_count > 0) or (best_s >= 0.45)\n",
    "            bibl_id    = best_id if (best_id and accept) else None\n",
    "            matched_by = \",\".join(best_reasons) if bibl_id else \"\"\n",
    "            score_val  = float(f\"{best_s:.3f}\") if bibl_id else 0.0\n",
    "        else:\n",
    "            bibl_id = None; matched_by = \"\"; score_val = 0.0\n",
    "\n",
    "        total, contexts = count_refs_and_contexts(root, bibl_id, valid_ids=all_ids)\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": tei.name.replace(\".tei.xml\",\"\"),\n",
    "            \"bibl_id\": bibl_id or \"\",\n",
    "            \"matched_by\": matched_by,\n",
    "            \"match_score\": f\"{score_val:.3f}\" if score_val else \"\",\n",
    "            \"total_intext\": total,\n",
    "        })\n",
    "\n",
    "        jout.write(json.dumps({\n",
    "            \"file\": tei.name.replace(\".tei.xml\",\"\"),\n",
    "            \"bibl_id\": bibl_id, \"matched_by\": matched_by,\n",
    "            \"match_score\": score_val, \"total_intext\": total,\n",
    "            \"contexts\": contexts\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# save CSVs\n",
    "with open(csv_counts, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"file\",\"bibl_id\",\"matched_by\",\"match_score\",\"total_intext\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "with open(csv_dbg, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"file\",\"rank\",\"bibl_id\",\"score\",\"reasons\",\"title\",\"authors\",\"year\",\"idnos_doi\",\"idnos_arxiv\",\"idnos_url\"])\n",
    "    w.writeheader(); w.writerows(match_debug)\n",
    "\n",
    "print(f\" Robust TEI counts saved:\\n- CSV: {csv_counts}\\n- JSONL: {jsonl_ctx}\\n- Debug: {csv_dbg}\\nFiles: {len(rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94863e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKEND = \"ollama\"\n",
    "OLLAMA_MODEL = \"phi3:mini\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: tei_contexts_robust_2025-09-13_08-58-28.jsonl and tei_counts_robust_2025-09-13_08-58-28.csv\n",
      "Windows extracted: 46 across 22 files\n",
      "Windows classified: 46\n",
      "✅ Saved:\n",
      "- CSV:   C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\stance_llm_2025-09-13_09-21-39.csv\n",
      "- JSONL: C:\\Users\\Karthik\\Desktop\\downloaded_files_2025-09-13_08-42-10\\grobid_outputs_2025-09-13_08-52-18\\stance_llm_evidence_2025-09-13_09-21-39.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karthik\\AppData\\Local\\Temp\\ipykernel_34868\\3293411824.py:189: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  agg = pred_df.groupby(\"file\").apply(assign_paper_label).reset_index()\n",
      "C:\\Users\\Karthik\\AppData\\Local\\Temp\\ipykernel_34868\\3293411824.py:207: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  for _, r in pred_df.groupby(\"file\").apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# === LLM stance classification using local Ollama (supportive / rebuttal / neutral) ===\n",
    "# Requirements:\n",
    "#   - Ollama running locally (http://localhost:11434)\n",
    "#   - Model pulled, e.g.: `ollama pull phi3:mini`  (or \"llama3:8b-instruct\" if your machine can handle it)\n",
    "#\n",
    "# What it does:\n",
    "#   1) Loads latest TEI contexts + counts from OUT_DIR\n",
    "#   2) Extracts citation windows (citing sentence ±1)\n",
    "#   3) Calls Ollama with a strict JSON instruction\n",
    "#   4) Aggregates per paper and saves CSV + JSONL\n",
    "#\n",
    "# Outputs:\n",
    "#   - stance_llm_<timestamp>.csv\n",
    "#   - stance_llm_evidence_<timestamp>.jsonl\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, json, time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "assert 'OUT_DIR' in globals() and OUT_DIR.exists(), \"Set OUT_DIR first (folder with TEI outputs).\"\n",
    "\n",
    "OLLAMA_HOST  = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = \"phi3:mini\"        # or \"llama3:8b-instruct\" (bigger, stronger)\n",
    "WINDOW_SIZE  = 1                  # citing sentence ±1 (0 = only the citing sentence)\n",
    "ONLY_FILES_MIN_CITES = 0          # set to 2 to classify only papers with >=2 in-text mentions\n",
    "MAX_WINDOWS_PER_FILE = 30         # cap per file to keep it fast; set None to disable\n",
    "CALL_PAUSE_S = 0.3                # gentle pacing between calls\n",
    "TIMEOUT_S    = 60                 # HTTP timeout per call\n",
    "\n",
    "# -------------- Load latest outputs --------------\n",
    "ctx_path    = sorted(OUT_DIR.glob(\"tei_contexts_robust_*.jsonl\"))[-1]\n",
    "counts_path = sorted(OUT_DIR.glob(\"tei_counts_robust_*.csv\"))[-1]\n",
    "print(\"Using:\", ctx_path.name, \"and\", counts_path.name)\n",
    "\n",
    "counts_df = pd.read_csv(counts_path)\n",
    "if ONLY_FILES_MIN_CITES > 0:\n",
    "    keep_files = set(counts_df.loc[counts_df[\"total_intext\"] >= ONLY_FILES_MIN_CITES, \"file\"])\n",
    "else:\n",
    "    keep_files = set(counts_df[\"file\"])\n",
    "\n",
    "# -------------- Utilities --------------\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Minimal, regex-only sentence splitter (fixed-width lookbehind).\n",
    "    Splits on [.?!] followed by space + capital or '('.\n",
    "    Normalizes patterns like 'text.) Next' -> 'text. Next'\n",
    "    \"\"\"\n",
    "    t = re.sub(r'\\s+', ' ', (text or '').strip())\n",
    "    if not t:\n",
    "        return []\n",
    "    t = re.sub(r'([.!?])\\s*\\)+', r'\\1 ', t)\n",
    "    parts = re.split(r'(?<=[.!?])\\s+(?=[A-Z(])', t)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def extract_cited_windows(paragraph: str, window: int = 1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return sentence windows centered on a sentence that looks like it contains a citation.\n",
    "    Numeric [..] or author-year (...) patterns.\n",
    "    Window=1 => prev + current + next as one string.\n",
    "    \"\"\"\n",
    "    sents = split_sentences(paragraph)\n",
    "    out = []\n",
    "    for i, s in enumerate(sents):\n",
    "        looks_numeric = bool(re.search(r'\\[\\s*\\d+(?:\\s*[,–-]\\s*\\d+)*\\s*\\]', s))\n",
    "        looks_authyr  = bool(re.search(r'\\([A-Z][A-Za-z\\-]+.*(?:19|20)\\d{2}[a-z]?\\)', s))\n",
    "        if looks_numeric or looks_authyr:\n",
    "            i0 = max(0, i - window); i1 = min(len(sents), i + 1 + window)\n",
    "            out.append(\" \".join(sents[i0:i1]))\n",
    "    # de-dup and keep order\n",
    "    seen, uniq = set(), []\n",
    "    for w in out:\n",
    "        k = re.sub(r'\\s+', ' ', w.strip())\n",
    "        if k and k not in seen:\n",
    "            seen.add(k); uniq.append(k)\n",
    "    return uniq\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You label the attitude of a citing passage toward a TARGET paper.\\n\"\n",
    "    \"Labels:\\n\"\n",
    "    \"- SUPPORTIVE: uses/adopts/extends/validates the TARGET’s method/findings.\\n\"\n",
    "    \"- REBUTTAL: contradicts, shows error/failure, provides counterexample, disputes claims.\\n\"\n",
    "    \"- NEUTRAL: background or mention without clear positive/negative judgment.\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Use ONLY the provided text window; judge stance toward the cited TARGET.\\n\"\n",
    "    \"- If ambiguous or mixed, choose NEUTRAL.\\n\"\n",
    "    \"Respond in compact JSON: {\\\"label\\\":\\\"supportive|rebuttal|neutral\\\",\\\"rationale\\\":\\\"<=25 words\\\"}.\"\n",
    ")\n",
    "\n",
    "def extract_json(text: str):\n",
    "    \"\"\"Robustly pull the first JSON object out of a model reply.\"\"\"\n",
    "    m = re.search(r'\\{.*\\}', text, flags=re.S)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def call_ollama_json(text: str, model: str = OLLAMA_MODEL, host: str = OLLAMA_HOST, retries: int = 2) -> dict:\n",
    "    \"\"\"\n",
    "    Call Ollama's local /api/chat and return a dict with keys: label, rationale.\n",
    "    On any error or malformed output, returns {'label':'neutral','rationale':'fallback'}.\n",
    "    \"\"\"\n",
    "    url = f\"{host}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
    "            {\"role\": \"user\",   \"content\": f\"Text:\\n{text}\"}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.0}\n",
    "    }\n",
    "    for r in range(retries + 1):\n",
    "        try:\n",
    "            resp = requests.post(url, json=payload, timeout=TIMEOUT_S)\n",
    "            resp.raise_for_status()\n",
    "            content = resp.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "            data = extract_json(content) or {}\n",
    "            label = str(data.get(\"label\", \"\")).strip().lower()\n",
    "            rationale = str(data.get(\"rationale\", \"\")).strip()\n",
    "            if label not in {\"supportive\", \"rebuttal\", \"neutral\"}:\n",
    "                label = \"neutral\"\n",
    "            return {\"label\": label, \"rationale\": rationale}\n",
    "        except Exception:\n",
    "            if r == retries:\n",
    "                return {\"label\": \"neutral\", \"rationale\": \"fallback (error)\"}\n",
    "            time.sleep(0.8 * (r + 1))\n",
    "\n",
    "def assign_paper_label(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Aggregate window-level labels into a paper-level label with a simple margin rule.\n",
    "    \"\"\"\n",
    "    sup = int((group[\"label\"] == \"supportive\").sum())\n",
    "    reb = int((group[\"label\"] == \"rebuttal\").sum())\n",
    "    neu = int((group[\"label\"] == \"neutral\").sum())\n",
    "    if reb >= sup + 1:\n",
    "        stance = \"rebuttal\"\n",
    "    elif sup >= reb + 1:\n",
    "        stance = \"supportive\"\n",
    "    else:\n",
    "        stance = \"neutral\"\n",
    "    return pd.Series({\n",
    "        \"supportive_windows\": sup,\n",
    "        \"rebuttal_windows\":   reb,\n",
    "        \"neutral_windows\":    neu,\n",
    "        \"stance_label\":        stance,\n",
    "        \"evidence_support\":   group.loc[group[\"label\"]==\"supportive\",\"text\"].head(3).tolist(),\n",
    "        \"evidence_rebuttal\":  group.loc[group[\"label\"]==\"rebuttal\",\"text\"].head(3).tolist(),\n",
    "    })\n",
    "\n",
    "# -------------- Build windows from contexts --------------\n",
    "windows = []  # rows: {file, text}\n",
    "with open(ctx_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        o = json.loads(line)\n",
    "        file = o[\"file\"]\n",
    "        if file not in keep_files:\n",
    "            continue\n",
    "        paras = o.get(\"contexts\", []) or []\n",
    "        file_windows = []\n",
    "        for p in paras:\n",
    "            file_windows.extend(extract_cited_windows(p, window=WINDOW_SIZE))\n",
    "        # cap per file (optional)\n",
    "        if MAX_WINDOWS_PER_FILE is not None and len(file_windows) > MAX_WINDOWS_PER_FILE:\n",
    "            file_windows = file_windows[:MAX_WINDOWS_PER_FILE]\n",
    "        for w in file_windows:\n",
    "            windows.append({\"file\": file, \"text\": w})\n",
    "\n",
    "win_df = pd.DataFrame(windows).drop_duplicates()\n",
    "print(f\"Windows extracted: {len(win_df)} across {win_df['file'].nunique()} files\")\n",
    "\n",
    "# -------------- Classify with Ollama --------------\n",
    "results = []\n",
    "for i, row in win_df.iterrows():\n",
    "    out = call_ollama_json(row[\"text\"])\n",
    "    results.append({\"file\": row[\"file\"], \"text\": row[\"text\"], **out})\n",
    "    if CALL_PAUSE_S > 0:\n",
    "        time.sleep(CALL_PAUSE_S)\n",
    "\n",
    "pred_df = pd.DataFrame(results)\n",
    "print(\"Windows classified:\", len(pred_df))\n",
    "\n",
    "# -------------- Aggregate to paper-level --------------\n",
    "if len(pred_df):\n",
    "    agg = pred_df.groupby(\"file\").apply(assign_paper_label).reset_index()\n",
    "else:\n",
    "    # handle empty gracefully\n",
    "    agg = pd.DataFrame(columns=[\"file\",\"supportive_windows\",\"rebuttal_windows\",\"neutral_windows\",\n",
    "                                \"stance_label\",\"evidence_support\",\"evidence_rebuttal\"])\n",
    "\n",
    "final = counts_df.merge(agg, on=\"file\", how=\"left\").fillna({\n",
    "    \"supportive_windows\":0, \"rebuttal_windows\":0, \"neutral_windows\":0, \"stance_label\":\"neutral\"\n",
    "})\n",
    "\n",
    "# -------------- Save --------------\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "stance_csv  = OUT_DIR / f\"stance_llm_{ts}.csv\"\n",
    "ev_jsonl    = OUT_DIR / f\"stance_llm_evidence_{ts}.jsonl\"\n",
    "\n",
    "final.to_csv(stance_csv, index=False)\n",
    "with open(ev_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    if len(pred_df):\n",
    "        for _, r in pred_df.groupby(\"file\").apply(lambda g: pd.Series({\n",
    "            \"file\": g[\"file\"].iloc[0],\n",
    "            \"evidence\": g[[\"text\",\"label\",\"rationale\"]].to_dict(orient=\"records\")\n",
    "        })).reset_index(drop=True).iterrows():\n",
    "            f.write(json.dumps(r._asdict() if hasattr(r, \"_asdict\") else dict(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved:\\n- CSV:   {stance_csv}\\n- JSONL: {ev_jsonl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citecheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
